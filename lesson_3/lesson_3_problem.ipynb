{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('/tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, and Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (1, 0) with action left: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action left: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (1, 0) with action left:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action left:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.9\n",
      "0.1\n",
      "0.9\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "state = env.sample(current_state, 0)\n",
    "print(state)\n",
    "\n",
    "print(env.T[current_state, 0, state])\n",
    "print(env.T[current_state, 1, state])\n",
    "print(env.T[current_state, 2, state])\n",
    "print(env.T[current_state, 3, state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*, *NiceLavaFloor*, and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach returns a matrix with the value for each state, the function *values_to_policy* automatically converts this matrix into the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    \n",
    "    while True:\n",
    "        maxiters -= 1\n",
    "        U = U_1.copy()\n",
    "        delta = 0\n",
    "        for state in range(environment.observation_space.n):\n",
    "            \n",
    "            max_array = [0 for _ in range(environment.action_space.n)] # for each possible action\n",
    "            for action in range(environment.action_space.n):\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    max_array[action] += env.T[state, action, next_state] * U[next_state]\n",
    "                    \n",
    "            if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                U_1[state] = environment.RS[state]\n",
    "            else:           \n",
    "                U_1[state] = environment.RS[state] + discount * max(max_array) # Bellman Update \n",
    "                \n",
    "            if abs(U_1[state] - U[state]) > delta: \n",
    "                delta = abs(U_1[state] - U[state])         \n",
    "                \n",
    "        if maxiters <= 0 or delta < (max_error*(1-discount)/discount):\n",
    "            break  \n",
    "  \n",
    "    return values_to_policy(np.asarray(U), environment)  # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.0059\n",
      "\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tValue Iteration  ########\u001b[0m\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92m===> Your solution is correct!\n",
      "\n",
      "Policy:\n",
      "[['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Policy Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*, *NiceLavaFloor*, and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "    # U_i = [0 for _ in range(environment.observation_space.n)]\n",
    "\n",
    "    while True:\n",
    "        U_i = [0 for _ in range(environment.observation_space.n)]\n",
    "        for _ in range(maxiters):\n",
    "            for _ in range(maxviter):\n",
    "                val = [0 for _ in range(environment.observation_space.n)]\n",
    "                for state in range(environment.observation_space.n):\n",
    "                    for next_state in range(environment.observation_space.n):\n",
    "                        val[state] += environment.T[state, policy[state], next_state] * U_i[next_state]\n",
    "                    \n",
    "                    if environment.grid[state] == 'G' or environment.grid[state] == 'P':\n",
    "                        U_i[state] = environment.RS[state]\n",
    "                    else: U_i[state] = environment.RS[state] + (discount * val[state])\n",
    "                        \n",
    "                    \n",
    "            U = U_i.copy()\n",
    "            unchanged = True\n",
    "            \n",
    "            for state in range(environment.observation_space.n):\n",
    "                # action\n",
    "                val_state = [0 for _ in range(environment.observation_space.n)]\n",
    "                val_action = [0 for _ in environment.actions]\n",
    "                for action in range(environment.action_space.n):\n",
    "                    for next_state in range(environment.observation_space.n):\n",
    "                        val_action[action] += environment.T[state, action, next_state] * U[next_state]\n",
    "                # policy\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    val_state[state] += environment.T[state, policy[state], next_state] * U[next_state]\n",
    "\n",
    "                if max(val_action) > val_state[state]:\n",
    "                    policy[state] = np.argmax(val_action)\n",
    "                    unchanged = False\n",
    "\n",
    "        if unchanged:\n",
    "            break\n",
    "    #       \n",
    "    # Code Here!\n",
    "    #\n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.0817\n",
      "\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tPolicy Iteration  ########\u001b[0m\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[92m===> Your solution is correct!\n",
      "\n",
      "Policy:\n",
      "[['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The following code compares Value Iteration and Policy Iteration by plotting the accumulated rewards of each episode with iterations in the range $[1, 50]$ (might take a long time if not optimized via NumPy). You can perform all the tests on different environment versions but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(environment, policy, max_iteration)** runs an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce6aa7ad0bf1507bb8840a7eab6e3548354d14fe4508c46a4e19f440cd08ed9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
